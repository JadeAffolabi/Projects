











# imports
import os

os.environ["DGLBACKEND"] = "pytorch"

import csv
import dgl
import torch
import torch
import pickle
import itertools
import numpy as np
import pandas as pd
import networkx as nx
import torch.nn as nn
import scipy.sparse as sparse
import matplotlib.pyplot as plt
import torch.nn.functional as F
from pyvis.network import Network
from sklearn.metrics import roc_auc_score
from dgl.dataloading import GraphDataLoader
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch.utils.data.sampler import SubsetRandomSampler





edges_data  = open('list_edges.csv', "r", encoding='utf8')
reader = csv.reader(edges_data)
edge_list = []
next(reader) # skip first line
for row in reader:
    edge_list.append((row[0],row[1]))


len(edge_list)


G = nx.DiGraph()
G.add_edges_from(edge_list)


print(f"Nodes: {len(G.nodes())}, Edges: {len(G.edges())}")


# Choose a subset of nodes to create a smaller graph for testing
subgraph_nodes = list(G.nodes())[:40]  # First 50 nodes for example
subgraph = G.subgraph(subgraph_nodes)

net = Network(notebook=True,cdn_resources="in_line", directed=True)
net.from_nx(subgraph)
net.force_atlas_2based()
net.save_graph("subgraph.html")


info_species = {}
with open('clean_species_and_taxo.csv', mode='r') as csv_file:
    csv_reader = csv.DictReader(csv_file,delimiter=";")
    line_count = 0
    next(csv_reader) #skip first row
    for row in csv_reader:
        info_species[row['Species ID']] = row


# Remove unnecessary information
keys_to_remove = ["Species ID", "ITIS code", "Catalogue of Life", "Species"]
for sp in info_species.keys():
    for key in keys_to_remove:
        info_species[sp].pop(key, None)


with open('data_species.pickle','rb') as file:
    data_species = pickle.load(file)


features_dict = data_species
for sp in info_species.keys():
    features_dict[sp].update(info_species[sp])
    features_dict[sp].pop("Sp.code", None)


excluded_species = []
for key in data_species.keys():
    if key not in info_species.keys():
        excluded_species.append(key)
print(f"The following species are not included in the graph : {excluded_species}")





# Remove species which are absent from list_edges.csv
for key in excluded_species:
        features_dict.pop(key, None)


features_df = pd.DataFrame(index=features_dict.keys(), columns=features_dict['A1'].keys())


for sp in features_dict.keys():
    for feat in features_df.columns:
        features_df.loc[sp, feat] = features_dict[sp][feat]


features_df.replace('NA', np.nan, inplace=True)
features_df.iloc[:,:35] = features_df.iloc[:,:35].astype(float)


features_df.iloc[:,35:38] = features_df.iloc[:,35:38].apply(LabelEncoder().fit_transform)


df = features_df
# Boolean mask of rows with at least one NaN
mask = df.isna().any(axis=1)

# Count of such rows
num_rows_with_nan = mask.sum()

# Indices of rows with at least one NaN
indices_with_nan = df.index[mask]

print("Number of rows with at least one NaN:", num_rows_with_nan)
print("Indices of those rows:", indices_with_nan.tolist())





#Remove the species with NaN values
features_df.drop(indices_with_nan, inplace=True)
features_df = features_df.astype(float)


#Remove edges with exclued species
new_edge_list = [edge for edge in edge_list if (edge[0] not in indices_with_nan) and (edge[1] not in indices_with_nan)]


lost_edges_ratio = 1-len(new_edge_list)/len(edge_list)
print(f"The deletion of species with NaN values induce a {100*lost_edges_ratio:.2f}% loss of edges.")


def print_feat_summary_family(family_letter):
    from collections import defaultdict
    # Create a nested dict: summary[subkey][value] = count
    summary = defaultdict(lambda: defaultdict(int))
    # Loop through keys starting with "A"
    for key, subdict in features_dict.items():
        if key.startswith(family_letter):
            for subkey, value in subdict.items():
                summary[subkey][value] += 1
    # Optional: convert nested defaultdicts to dicts for pretty printing
    summary = {k: dict(v) for k, v in summary.items()}
    for key, value in summary.items():
        print(f"{key} : {value}")



features_df





dgl_node_IDs = {species : index for index, species in enumerate(features_df.index)}


dgl_node_2_species = {index : species for index, species in enumerate(features_df.index)}


source_nodes = []
targert_nodes = []
for edge in new_edge_list:
    source_nodes.append(dgl_node_IDs[edge[0]])
    targert_nodes.append(dgl_node_IDs[edge[1]])
source_nodes_tensor = torch.tensor(source_nodes)
targert_nodes_tensor = torch.tensor(targert_nodes)


trophic_graph = dgl.graph((source_nodes_tensor, targert_nodes_tensor))
trophic_graph.ndata['feat'] = torch.tensor(features_df.values, dtype=torch.float32)








src, dst = trophic_graph.edges() # src = source, dst = destination
num_nodes = trophic_graph.num_nodes()
edge_ids = np.arange(trophic_graph.num_edges())
train_sample, test_sample = train_test_split(edge_ids, test_size=0.1,
                                             shuffle=True, random_state=42)
train_sample, val_sample = train_test_split(train_sample, test_size=0.2,
                                            shuffle=True, random_state=42)
# Positive edges sampling
pos_test_src, pos_test_dst = src[test_sample], dst[test_sample]
pos_val_src, pos_val_dst = src[val_sample], dst[val_sample]
pos_train_src, pos_train_dst = src[train_sample], dst[train_sample]

# Negative edges sampling
coo_mat = sparse.coo_matrix((np.ones(len(src)), (src.numpy(), dst.numpy())),
                        shape=(num_nodes, num_nodes))
neg_adj = 1 - coo_mat.todense() - np.eye(num_nodes)
neg_src, neg_dst = np.where(neg_adj != 0)

neg_edge_ids = np.arange(len(neg_src))
neg_train_sample, neg_test_sample = train_test_split(neg_edge_ids, test_size=0.1,
                                             shuffle=True, random_state=42)
neg_train_sample, neg_val_sample = train_test_split(neg_train_sample, test_size=0.2,
                                            shuffle=True, random_state=42)

neg_test_src, neg_test_dst = neg_src[neg_test_sample], neg_dst[neg_test_sample]
neg_val_src, neg_val_dst = neg_src[neg_val_sample], neg_dst[neg_val_sample]
neg_train_src, neg_train_dst = neg_src[neg_train_sample], neg_dst[neg_train_sample]

train_graph = dgl.remove_edges(trophic_graph, np.concatenate((test_sample, val_sample)))
test_graph = dgl.remove_edges(trophic_graph, np.concatenate((train_sample, val_sample)))
val_graph = dgl.remove_edges(trophic_graph, np.concatenate((train_sample, test_sample)))





train_pos_graph = dgl.graph((pos_train_src, pos_train_dst), num_nodes=num_nodes)
train_neg_graph = dgl.graph((neg_train_src, neg_train_dst), num_nodes=num_nodes)

val_pos_graph = dgl.graph((pos_val_src, pos_val_dst), num_nodes=num_nodes)
val_neg_graph = dgl.graph((neg_val_src, neg_val_dst), num_nodes=num_nodes)

test_pos_graph = dgl.graph((pos_test_src, pos_test_dst), num_nodes=num_nodes)
test_neg_graph = dgl.graph((neg_test_src, neg_test_dst), num_nodes=num_nodes)



class Dataloader_pos_neg_graph():
    def __init__(self, pos_g, neg_g, batch_size):
        self.pos_graph = pos_g
        self.pos_num_edges = pos_g.num_edges()
        self.neg_num_edges = neg_g.num_edges()
        self.neg_graph = neg_g
        self.num_batches = pos_g.num_edges()//batch_size

    def __iter__(self):
        for pos_indices, neg_indices in zip(torch.randperm(self.pos_num_edges).chunk(self.num_batches),
                                            torch.randperm(self.neg_num_edges).chunk(self.num_batches)):
            yield self.pos_graph.edge_subgraph(pos_indices), self.neg_graph.edge_subgraph(neg_indices)





from dgl.nn import SAGEConv, GraphConv, GATConv
import dgl.function as fn

class GraphSAGE(nn.Module):
     def __init__(self, in_feats, h_feats):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats, "mean")
        self.conv2 = SAGEConv(h_feats, h_feats, "mean")
        self.drop = nn.Dropout(0.2)

     def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.drop(h)
        h = self.conv2(g, h)
        return h


class GCN(nn.Module):
     def __init__(self, in_feats, h_feats):
        super(GCN, self).__init__()
        self.conv1 = GraphConv(in_feats, h_feats)
        self.conv2 = GraphConv(h_feats, h_feats)

     def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h

class GAT(nn.Module):
     def __init__(self, in_feats, h_feats):
        super(GAT, self).__init__()
        self.conv1 = GATConv(in_feats, h_feats, num_heads=1)
        self.conv2 = GATConv(h_feats, h_feats, num_heads=1)

     def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h


class DotPredictor(nn.Module):
    def forward(self, g, h):
        with g.local_scope():
            g.ndata['h'] = h
            g.apply_edges(fn.u_dot_v("h", "h", "score"))
            #u_dot_v returns a 1-element vector for each edge so you need to squeeze it.
            return g.edata["score"][:, 0]

class MLPPredictor(nn.Module):
    def __init__(self, h_feats):
        super().__init__()
        self.W1 = nn.Linear(h_feats * 2, 128)
        self.W2 = nn.Linear(128, 64)
        self.W3 = nn.Linear(64, 1)

    def forward_linear(self, h):
      h = F.relu(self.W1(h))
      h = F.relu(self.W2(h))
      score = self.W3(h)
      return score

    def apply_edges(self, edges):
        h = torch.cat([edges.src["h"], edges.dst["h"]], 1)
        return {"score": self.forward_linear(h).squeeze(1)}

    def forward(self, g, h):
        with g.local_scope():
            g.ndata["h"] = h
            g.apply_edges(self.apply_edges)
            return g.edata["score"]


def suppress_zero_degree(batch):
    graphs = dgl.unbatch(batch)
    new_graphs = [dgl.add_self_loop(g) for g in graphs]
    return dgl.batch(new_graphs)



def compute_loss(pos_score, neg_score, device):
    scores = torch.cat([pos_score, neg_score])
    labels = torch.cat([torch.ones(pos_score.shape[0]),
                        torch.zeros(neg_score.shape[0])]).squeeze()
    labels = labels.to(device)
    return F.binary_cross_entropy_with_logits(scores, labels)

def compute_auc(pos_score, neg_score):
    scores =  torch.cat([pos_score, neg_score]).numpy()
    labels = torch.cat([torch.ones(pos_score.shape[0]),
                        torch.zeros(neg_score.shape[0])]).numpy()
    return roc_auc_score(labels, scores)


def plot_loss(loss_history):
    plt.plot(loss_history["Train"][2:], label="Train loss")
    plt.plot(loss_history["Val"][2:], label="Val loss")
    plt.xlabel("Epoch")
    plt.ylabel("BCE Loss")
    plt.grid()
    plt.legend()
    plt.show()


def topk_prediction_errors(pos_graph, neg_graph, model, predictor, device, k=10):
    # Predictions
    pos_pred = predictor(pos_graph.to(device), h)
    neg_pred = predictor(neg_graph.to(device), h)

    scores = torch.cat([pos_pred.cpu(), neg_pred.cpu()])
    labels = torch.cat([
        torch.ones_like(pos_pred.cpu()),
        torch.zeros_like(neg_pred.cpu())
    ])

    pos_src, pos_dst = pos_graph.edges()
    neg_src, neg_dst = neg_graph.edges()
    src = torch.cat([pos_src, neg_src])
    dst = torch.cat([pos_dst, neg_dst])

    sorted_indices = np.argsort(np.abs(scores.numpy()))[::-1]
    fp = []
    fn = []

    for idx in sorted_indices:
        if labels[idx] == 0 and len(fp) < 10:
            fp.append((idx, scores[idx], labels[idx]))  # false positive
        elif labels[idx] == 1 and len(fn) < 10:
            fn.append((idx, scores[idx], labels[idx]))  # false negative
        if len(fp) >= 10 and len(fn) >= 10:
            break
    print("\nTop 10 sur les liens inexistants:")
    for i, score, label in fp:
        u = int(src[i])
        v = int(dst[i])
        print(f"Lien ({dgl_node_2_species[u]}, {dgl_node_2_species[v]}) \n Score: {score:.4f}, True Label: {label}")
        print("-" * 40)


    print("\nTop 10 sur les liens qui existent:")
    for i, score, label in fn:
        u = int(src[i])
        v = int(dst[i])
        print(f"Lien ({dgl_node_2_species[u]}, {dgl_node_2_species[v]}) \n Score: {score:.4f}, True Label: {label}")
        print("-" * 40)



device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
h_feats = 24
model = GraphSAGE(train_graph.ndata["feat"].shape[1], h_feats).to(device)
predictor = MLPPredictor(h_feats).to(device)
#predictor = DotPredictor().to(device)
optimizer = torch.optim.Adam(itertools.chain(model.parameters(), predictor.parameters()),
                             lr=0.01)
EPOCHS = 300
BATCH_SIZE = 512
loss_history = {'Train':[], 'Val':[]}
train_graph = train_graph.to(device)
test_graph = test_graph.to(device)
val_graph = val_graph.to(device)
for epoch in range(EPOCHS):
    model.train()
    # Forward
    h = model(train_graph, train_graph.ndata["feat"])
    pos_pred = predictor(train_pos_graph.to(device), h)
    neg_pred = predictor(train_neg_graph.to(device), h)
    loss = compute_loss(pos_pred, neg_pred, device)
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_history['Train'].append(loss.item())

    # Validation
    model.eval()
    with torch.no_grad():
      h = model(val_graph, val_graph.ndata["feat"])
      pos_pred = predictor(val_pos_graph.to(device), h)
      neg_pred = predictor(val_neg_graph.to(device), h)
      loss = compute_loss(pos_pred, neg_pred, device)
      loss_history['Val'].append(loss.item())

    if epoch%5 == 0:
        print(f"In epoch {epoch}, loss: {loss:.3f}")

plot_loss(loss_history)

# Prediction
model.eval()
with torch.no_grad():
    h = model(test_graph, test_graph.ndata["feat"])
    pos_pred = predictor(test_pos_graph.to(device), h)
    neg_pred = predictor(test_neg_graph.to(device), h)
    print("Test evaluation")
    print("AUC", compute_auc(pos_pred.cpu(), neg_pred.cpu()))



model.eval()
with torch.no_grad():
  h = model(test_graph, test_graph.ndata["feat"])
  topk_prediction_errors(test_pos_graph, test_neg_graph, h, predictor, device)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
h_feats = 24
model = GraphSAGE(train_graph.ndata["feat"].shape[1], h_feats).to(device)
predictor = MLPPredictor(h_feats).to(device)
#predictor = DotPredictor().to(device)
optimizer = torch.optim.Adam(itertools.chain(model.parameters(), predictor.parameters()),
                             lr=0.01)
EPOCHS = 500
BATCH_SIZE = 512
loss_history = {'Train':[], 'Val':[]}
train_graph = train_graph.to(device)
test_graph = test_graph.to(device)
val_graph = val_graph.to(device)
for epoch in range(EPOCHS):
    model.train()
    # Forward
    h = model(train_graph, train_graph.ndata["feat"])
    pos_pred = predictor(train_pos_graph.to(device), h)
    neg_pred = predictor(train_neg_graph.to(device), h)
    loss = compute_loss(pos_pred, neg_pred, device)
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_history['Train'].append(loss.item())

    # Validation
    model.eval()
    with torch.no_grad():
      h = model(val_graph, val_graph.ndata["feat"])
      pos_pred = predictor(val_pos_graph.to(device), h)
      neg_pred = predictor(val_neg_graph.to(device), h)
      loss = compute_loss(pos_pred, neg_pred, device)
      loss_history['Val'].append(loss.item())

    if epoch%5 == 0:
        print(f"In epoch {epoch}, loss: {loss:.3f}")

plot_loss(loss_history)

# Prediction
model.eval()
with torch.no_grad():
    h = model(test_graph, test_graph.ndata["feat"])
    pos_pred = predictor(test_pos_graph.to(device), h)
    neg_pred = predictor(test_neg_graph.to(device), h)
    print("Test evaluation")
    print("AUC", compute_auc(pos_pred.cpu(), neg_pred.cpu()))


model.eval()
with torch.no_grad():
  h = model(test_graph, test_graph.ndata["feat"])
  topk_prediction_errors(test_pos_graph, test_neg_graph, h, predictor, device)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
h_feats = 24
model = GCN(train_graph.ndata["feat"].shape[1], h_feats).to(device)
predictor = MLPPredictor(h_feats).to(device)
#predictor = DotPredictor().to(device)
optimizer = torch.optim.Adam(itertools.chain(model.parameters(), predictor.parameters()),
                             lr=0.001)
EPOCHS = 500
BATCH_SIZE = 512
loss_history = {'Train':[], 'Val':[]}
train_graph = train_graph.to(device)
test_graph = test_graph.to(device)
val_graph = val_graph.to(device)
for epoch in range(EPOCHS):
    model.train()
    # Forward
    new_train_graph = suppress_zero_degree(train_graph)
    h = model(new_train_graph, train_graph.ndata["feat"])
    pos_pred = predictor(train_pos_graph.to(device), h)
    neg_pred = predictor(train_neg_graph.to(device), h)
    loss = compute_loss(pos_pred, neg_pred, device)
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_history['Train'].append(loss.item())

    # Validation
    model.eval()
    with torch.no_grad():
      new_val_graph = suppress_zero_degree(val_graph)
      h = model(new_val_graph, val_graph.ndata["feat"])
      pos_pred = predictor(val_pos_graph.to(device), h)
      neg_pred = predictor(val_neg_graph.to(device), h)
      loss = compute_loss(pos_pred, neg_pred, device)
      loss_history['Val'].append(loss.item())

    if epoch%5 == 0:
        print(f"In epoch {epoch}, loss: {loss:.3f}")

plot_loss(loss_history)

# Prediction
model.eval()
with torch.no_grad():
    new_test_graph = suppress_zero_degree(test_graph)
    h = model(new_test_graph, test_graph.ndata["feat"])
    pos_pred = predictor(test_pos_graph.to(device), h)
    neg_pred = predictor(test_neg_graph.to(device), h)
    print("Test evaluation")
    print("AUC", compute_auc(pos_pred.cpu(), neg_pred.cpu()))


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
h_feats = 24
model = GraphSAGE(train_graph.ndata["feat"].shape[1], h_feats).to(device)
predictor = MLPPredictor(h_feats).to(device)
#predictor = DotPredictor().to(device)
optimizer = torch.optim.Adam(itertools.chain(model.parameters(), predictor.parameters()),
                             lr=0.01)
EPOCHS = 200
BATCH_SIZE = 512
loss_history = {'Train':[], 'Val':[]}
train_graph = train_graph.to(device)
test_graph = test_graph.to(device)
val_graph = val_graph.to(device)
for epoch in range(EPOCHS):
    model.train()
    train_dataloader = Dataloader_pos_neg_graph(train_pos_graph, train_neg_graph, BATCH_SIZE)
    total_train_loss = 0
    for batch_pos, batch_neg in train_dataloader:
        # Forward
        h = model(train_graph, train_graph.ndata["feat"])
        pos_pred = predictor(batch_pos.to(device), h[batch_pos.ndata[dgl.NID]])
        neg_pred = predictor(batch_neg.to(device), h[batch_neg.ndata[dgl.NID]])
        loss = compute_loss(pos_pred, neg_pred, device)
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()
    loss_history['Train'].append(total_train_loss/train_dataloader.num_batches)

    # Evaluation on the validation set
    model.eval()
    with torch.no_grad():
      val_dataloader = Dataloader_pos_neg_graph(val_pos_graph, val_neg_graph, BATCH_SIZE)
      total_val_loss = 0
      for batch_pos, batch_neg in val_dataloader:
        h = model(val_graph, val_graph.ndata["feat"])
        pos_pred = predictor(batch_pos.to(device), h[batch_pos.ndata[dgl.NID]])
        neg_pred = predictor(batch_neg.to(device), h[batch_neg.ndata[dgl.NID]])
        loss = compute_loss(pos_pred, neg_pred, device)
        total_val_loss += loss.item()
      loss_history['Val'].append(total_val_loss/val_dataloader.num_batches)

    if epoch%5 == 0:
        print(f"In epoch {epoch}, loss: {total_train_loss/train_dataloader.num_batches:.3f}")

plot_loss(loss_history)

# Evaluation on the test set
model.eval()
with torch.no_grad():
    h = model(test_graph, test_graph.ndata["feat"])
    pos_pred = predictor(test_pos_graph.to(device), h)
    neg_pred = predictor(test_neg_graph.to(device), h)
    print("Test evaluation")
    print("AUC", compute_auc(pos_pred.cpu(), neg_pred.cpu()))
